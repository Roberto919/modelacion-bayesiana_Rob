
# Diagnósticos para algoritmos de simulación


```{r, include=FALSE, message=FALSE}
library(tidyverse)
library(patchwork)
library(scales)
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning=FALSE, 
                      fig.align = 'center', fig.width = 5, fig.height=3, cache = TRUE)
comma <- function(x) format(x, digits = 2, big.mark = ",")
theme_set(theme_linedraw())
color.blues <- c(NA,"#BDD7E7", "#6BAED6", "#3182BD", "#08519C", "#074789", "#063e77", "#053464")
color.itam  <- c("#00362b", "#00503f", "#006953", "#008367", "#009c7b", "#00b68f")


sin_lineas <- theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank())
sin_leyenda <- theme(legend.position = "none")
sin_ejes <- theme(axis.ticks = element_blank(), 
        axis.text = element_blank())
```

El método de MCMC es un método iterativo estócastico con el cual podemos
responder preguntas de inferencia estadística. En particular, ha permitido la
adopción de inferencia Bayesiana en distintas aplicaciones y áreas de la
ciencia.

Las dos complicaciones más importantes a las que se enfrenta son: 

- Si el procedimiento sólo ha sido implementado un número muy pequeño de
iteraciones, entonces no hay garantía de que las simulaciones sean
representativas de la distribución objetivo (cuadro izquierdo superior en la
figura de abajo).

- La dependencia dentro de la cadena puede ser muy fuerte y entonces, aparte de
los problemas obvios de convergencia, tendremos estimadores Monte Carlo menos
precisos que con el mismo número de simulaciones independientes (cuadros
inferiores en la figura de abajo)

```{r, echo = FALSE}

library(mvtnorm)

crear_metropolis <- function(fun_log, sigma_salto = 0.1){
  # fun_log es la funcion objetivo en escaa logaritmica
  # sigma_salto es la varianza de los saltos en la propuesta
  iterar_metropolis <- function(theta_inicial, n){
    p <- length(theta_inicial)
    nombres <- names(theta_inicial)
    iteraciones <- matrix(0, nrow = n, ncol = p)
    colnames(iteraciones) <- nombres
    iteraciones[1,] <- theta_inicial
    for(i in 2:n){
      theta <- iteraciones[i - 1, ]
      theta_prop <- theta + rnorm(p, 0, sigma_salto)
      # exp(log(p) - log(q)) = p/q
      cociente <- exp(fun_log(theta_prop) - fun_log(theta))
      if(cociente >= 1 || runif(1,0,1) < cociente){
        iteraciones[i, ] <- theta_prop
      } else {
        iteraciones[i, ] <- theta  
      }
    }
    iteraciones_tbl <- iteraciones %>% 
      as_tibble() %>%  
      mutate(iter_num = row_number()) %>% 
      select(iter_num, everything())
    iteraciones_tbl
  }
  iterar_metropolis
}

```


```{r, echo = FALSE, out.width = "99%", fig.asp = .6, fig.height = 8}

set.seed(1087)

mu <- c(0, 0)
Sigma <- matrix(c(1, .75, .75, 1), nrow = 2)

crear_log_norm <- function(mu, Sigma){
  # calcula log_posterior
  log_norm <- function(x){
      log_verosim <- dmvnorm(x, mu, Sigma, log = TRUE)
  }
  log_norm
}

log_norm <- crear_log_norm(mu, Sigma)

corre_metropolis <- function(x1, x2){
  
  n_muestras <- 1000
  metro_normal <- crear_metropolis(log_norm, sigma_salto = .15)
  metro_normal(c(x1 = x1, x2 = x2), n_muestras) 
  
}

cadenas <- tibble(x0 = c(-2, 2, 2, -2, 0), y0 = c(2, -2, 2, -2, 0), id = seq(1,5)) %>% 
  mutate(cadena = map2(x0, y0, corre_metropolis), 
         id = factor(id)) %>%
  unnest(cadena)

g_cadenas_cortas <- cadenas %>% 
  filter(iter_num <= 50) %>% 
  ggplot(aes(x = x1, y = x2, group = id, color  = id)) + 
      geom_path() + geom_point(size = .3) + 
      geom_point(aes(x0, y0), color = 'red') + 
      xlab(expression(x[1])) + ylab(expression(x[2])) + 
      sin_lineas + sin_leyenda

g_cadenas_largas <- cadenas %>% 
  ggplot(aes(x = x1, y = x2, group = id, colour = id)) + 
      geom_path() + geom_point(size = .3) + 
      geom_point(aes(x0, y0), color = 'red') + 
      xlab(expression(x[1])) + ylab(expression(x[2])) + 
      sin_lineas + sin_leyenda

g_cadenas_conjunta <- cadenas %>% 
  ggplot(aes(x = x1, y = x2, group = id)) + 
      geom_point( size =.3) + 
      geom_point(aes(x0, y0), color = 'red') + 
      xlab(expression(x[1])) + ylab(expression(x[2])) + 
      sin_lineas + sin_leyenda

g_independientes <- as.tibble(rmvnorm(5000, mean = mu, sigma = Sigma)) %>% 
  ggplot(aes(x = V1, y = V2)) + 
      geom_point( size =.3) + 
      xlab(expression(x[1])) + ylab(expression(x[2])) + 
      sin_lineas + sin_leyenda

g_cadenas_cortas + g_cadenas_largas + g_cadenas_conjunta + g_independientes

```

Por lo tanto, cuando generamos una muestra de la distribución posterior usando
MCMC, sin importar el método (Metrópolis, Gibbs, HMC), buscamos que:

1. Los valores simulados no están influenciados por el valor inicial
(arbitrario) y deben explorar todo el rango de la posterior, con suficientes
retornos para evaluar cuánta masa hay en cada región.

2. Debemos tener suficientes simulaciones de tal manera que las
estimaciones sean precisas y estables.

3. Queremos tener métodos y resúmenes informativos que nos ayuden diagnosticar
correctamente el desempeño de nuestas simulaciones.

En la práctica intentamos cumplir lo más posible estos objetivos, pues aunque en
principio los métodos MCMC garantizan que una cadena infinitamente larga logrará
una representación perfecta, siempre debemos tener un criterio para cortar la
cadena y evaluar la calidad de las simulaciones.

Primero estudiaremos diagnósticos generales para métodos que utilicen MCMC y
después estudiaremos particularidades del método de simulación HMC (pues es el 
que utilizaremos a través de Stan).

## Diagnósticos generales {-}

Una forma que tenemos de evaluar la (o identificar la falta de) convergencia es
considerar distintas secuencias independientes. La inferencia se realizará con 
las simulaciones de todas las cadenas en conjunto (cuadro inferior izquierdo en
la figura de arriba) una vez desechados los estados de la cadena que sean
influenciados por los puntos iniciales.

```{r, echo = FALSE}

corre_metropolis <- function(x1, x2){
  
  n_muestras <- 1000
  metro_normal <- crear_metropolis(log_norm, sigma_salto = 1.68)
  metro_normal(c(x1 = x1, x2 = x2), n_muestras) 
  
}

cadenas_bien <- tibble(x0 = c(-2, 2, 2, -2, 0), y0 = c(2, -2, 2, -2, 0), id = seq(1,5)) %>% 
  mutate(cadena = map2(x0, y0, corre_metropolis), 
         id = factor(id)) %>%
  unnest(cadena)

```



```{r, echo = FALSE}

trace_corr <- cadenas %>% 
  ggplot(aes(x = iter_num, y = x1, color = id)) + 
    geom_path() + sin_leyenda + sin_lineas

trace_uncorr <- cadenas_bien %>% 
  ggplot(aes(x = iter_num, y = x1, color = id)) + 
    geom_path() + sin_leyenda + sin_lineas

trace_corr / trace_uncorr

```

### Monitoreo de convergencia {-}

**Burn-in e iteraciones iniciales.** En primer lugar, en muchas ocasiones las
condiciones iniciales de las cadenas las escogemos de tal forma que 
que son valores "atípicos" en términos de la posterior. Estrategias de selección
de puntos iniciales pueden ser valores aleatorios de la previa o perturbaciones 
aleatorias a estimadores $\textsf{MLE}$.

Correr varias cadenas en puntos dispersos tienen la ventaja de explorar desde
distintas regiones de la posterior. Eventualmente, esperamos que todas las
cadenas mezclen bien y representen realizaciones independientes del mismo
proceso estócastico (Markoviano). Es decir, procesos de difusión de la misma
distribución.

Para contrarrestar la dependencia en los distintos puntos iniciales se descarta 
parte de la cadena en un periodo inicial (iteraciones de *burn-in*).

Por ejemplo, para el ejemplo de los cantantes, podemos ver que las iteraciones
iniciales tienen como función principal llegar a las regiones de probabilidad
posterior alta:

```{r, out.width = "99%"}
log_p <- crear_log_posterior_norm(cantantes$estatura_cm, mu_0, n_0, a, b) 
log_post <- function(pars) { log_p(pars[1], pars[2]) }
set.seed(823)
metro_normal <- crear_metropolis(log_post, sigma_salto = 0.5)
sim_tbl <- metro_normal(c(mu = 162, sigma = 1), 5000) 
ggplot(sim_tbl %>% filter(iter_num < 500), aes(x = mu, y = sigma)) + 
  geom_path(alpha = 0.5) + geom_point(aes(colour = iter_num)) + 
  sin_lineas
```

```{r, fig.width=7}
sim_g <- sim_tbl %>% pivot_longer(-iter_num, 
                                    names_to = "parametro",
                                    values_to = "valor")
todas <- ggplot(sim_g, aes(x = iter_num, y = valor)) +
  geom_line(alpha = 0.5) +
  facet_wrap(~ parametro, ncol = 1, scales = "free_y") +
  labs(subtitle = "Todas las simulaciones") + sin_lineas
sin_burnin <- 
  sim_g %>% filter(iter_num > 200) %>% 
  ggplot(aes(x = iter_num, y = valor)) +
  geom_line(alpha = 0.5) +
  facet_wrap(~ parametro, ncol = 1, scales = "free_y") +
  labs(subtitle = "Quitando 200 de burn-in") + sin_lineas
todas + sin_burnin
```

@bda recomiendan descartar la mitad de las iteraciones de cada una de las cadenas
que se simularon. Para problemas en dimensiones altas, incluso se podría esperar 
descartar hasta un 80\% de simulaciones (en especial para métodos basados en
Metropolis-Hastings).

Por ejemplo, consideremos la evolución de cadenas de Markov siguiente.
Independientemente, cada una de las cadenas podrían parecer haber alcanzado un
estado estacionario. Sin embargo, al considerar que cada una de las cadenas
debería ser una realización del mismo proceso, nos damos cuenta que aun no
han alcanzado el estado estacionario.

```{r, fig.height = 3, out.width = "99%", fig.asp = .4}
set.seed(8513)
valores_iniciales  <- tibble(mu_0 = rnorm(4, 160, 20), 
                             sigma_0 = runif(4, 0, 20),
                             cadena = as.factor(1:4))
sims_tbl <- valores_iniciales %>% 
  mutate(sims = map2(mu_0, sigma_0, 
    ~ metro_normal(c(mu = .x, sigma = .y), 300) )) %>% 
  unnest(sims)

ggplot(sims_tbl, aes(x = iter_num, y = sigma, colour = cadena)) +
  geom_line() + sin_lineas
```

El problema es que tienen muy distintas medias y varianzas. Por ejemplo si
consideramos:

```{r, fig.height = 3, out.width = "99%", fig.asp = .4}
set.seed(83243)
sims_tbl <- valores_iniciales %>% 
  mutate(sims = map2(mu_0, sigma_0, 
    ~ metro_normal(c(mu = .x, sigma = .y), 20000) )) %>% 
  unnest(sims)

ggplot(sims_tbl, aes(x = iter_num, y = sigma, colour = cadena)) +
  geom_line() + sin_lineas
```

Este resultado se ve mejor. La parte *transición* hacia las zonas
de alta probabilidad pasa antes de unas 1000 iteraciones. Podemos
hacer más simulaciones, o eliminar como *burn-in* las primeras iteraciones:

```{r, fig.width = 6}
media_g <- ggplot(sims_tbl %>% filter(iter_num > 2000),
                  aes(x = iter_num, y = mu, colour = factor(cadena))) +
  geom_line() + sin_lineas
sigma_g <- ggplot(sims_tbl %>% filter(iter_num > 2000),
                  aes(x = iter_num, y = sigma, colour = factor(cadena))) +
  geom_line() + sin_lineas
media_g / sigma_g
```

Las gráficas anteriores nos ayudan a determinar si elegimos un periodo de 
calentamiento adecuado o si alguna cadena está alejada del resto.

Una vez que las cadenas están en estado estable, podemos usar
**todas** las simulaciones juntas para resumir:

```{r}
head(sims_tbl)
# medias posteriores
sims_tbl %>% 
  summarise(mu = mean(mu), sigma = mean(sigma))
```

El problema de estos diagnósticos gráficos es que están sujetos a nuestra
capacidad de analizar cada una de los parámetros para identificar malas
transiciones y/o evolución de las cadenas. En problemas de altas dimensiones
esto es una limitante. 

### Monitoreando la mezcla dentro y entre cadenas {-}

Gelman y diversos de sus coatures han desarollado un diagnóstico numérico para evaluar
implementaciones de MCMC al considerar múltiples cadenas. Aunque éste
estadístico se ha ido refinando con los años, su desarrollo muestra 
un entendimiento gradual de éstos métodos en la práctica. La
medida $\hat{R}$ se conoce como el **factor de reducción potencial de escala**.
Éste pretende ser una estimación de la posible reducción en la longitud de un
intervalo de confianza si las simulaciones continuaran infinitamente. 

El diagnostico que discutiremos estudia de manera simultánea la mezcla de todas
las cadenas (cada cadena, y fracciones de ella, debería de haber transitado el
soporte de la distribución objetivo) y estacionalidad (de haberse logrado cada
mitad de una cadena deberían de poseer las mismas estadísticas).

La estrategia es descartar la primera mitad de cada cadena. Es decir,
consideremos 5 cadenas de 1000 iteraciones. De cada cadena descartamos las
primeras 500 como *burn-in*. El resto lo volvemos a dividir en dos y utilizamos
cada fracción como si fuera una cadena. Al final tendremos 10 cadenas de
longitud 250.

Ahora, denotemos por $m$ el número de cadenas simuladas y por $n$ el número de 
simulaciones dentro de cada cadena. Cada una de las cantidades escalares de
interés las denotamos por $\phi.$ Éstas pueden ser los parámetros originales
$\theta$ o alguna otra cantidad derivada $\phi = f(\theta).$ Ahora denotemos por
$\phi_{ij}$ las simulaciones que tenemos disponibles con $i = 1, \ldots, n$, y
$j = 1, \ldots, m.$ Calculamos $B$ y $W$, la variabilidad entre (_between_) y
dentro (_within_) cadenas, respectivamente, por medio de 
\begin{subequations}
\begin{align}
W &= \frac1m \sum_{j = 1}^m s_j^2, \quad \text{con} \quad s_j^2 = \frac{1}{n-1}\sum_{i = 1}^n (\phi_{ij} - \bar \phi_{\cdot j})^2, \quad \text{donde} \quad \bar \phi_{\cdot j} = \frac1n \sum_{i = 1}^n \phi_{ij}, \\
B &= \frac{n}{m-1}\sum_{j = 1}^m (\bar \phi_{\cdot j} - \bar \phi_{\cdot \cdot})^2, \quad \text{donde} \quad \bar \phi_{\cdot \cdot} = \frac1m \sum_{j = 1}^m \bar \phi_{\cdot j}.
\end{align}
\end{subequations}

La varianza entre cadenas, $B$, se multiplica por $n$ dado que ésta se calcula
por medio de promedios y sin este factor de corrección no reflejaría la
variabilidad de las cantidades de interés $\phi.$

La varianza de $\phi$ se puede estimar por medio de 

$$
\hat{\mathbb{V}}(\phi)^+ = \frac{n -1}{n} W + \frac{1}{n} B \, .
$$

Nota que este estimador sobre-estima la varianza pues los puntos iniciales
pueden estar sobre-dispersos, mientras que es un estimador insesgado una vez
que se haya alcanzado el estado estacionario (realizaciones de la distribución
objetivo), o en el límite $n \to \infty.$ Por otro lado, la varianza
estimada por $W$ será un sub-estimador pues podría ser el caso de que cada
cadena no ha tenido la oportunidad de recorrer todo el soporte de la
distribución. En el límite $n \to \infty,$ el valor esperado de $W$ aproxima
$\mathbb{V}(\phi).$


Al final, se utiliza como diagnostico el factor por el cual la escala de la
distribución actual de $\phi$ se puede reducir si se continua con el
procedimiento en el límite $n \to \infty.$ Esto es, 

$$\hat{R} = \sqrt{\frac{\hat{\mathbb{V}}(\phi)^+}{W}}\,,$$

quien por construcción converge a 1 cuando $n \to \infty.$ Si el estimador
presenta valores altos, entonces tenemos indicios de que se puede mejorar la
estimación de varianzas si se continua con un mayor número de simulaciones.

Por ejemplo, consideremos nuestro ejemplo de las normales. Las cadenas que son
altamente correlacionadas (longitud de salto pequeño) presentan los siguientes
estadísticos.

```{r, echo = FALSE, fig.height = 4, out.width = "99%"}

diag_corr <- cadenas %>% 
  filter(iter_num > 500) %>% 
  mutate(cadena = paste(id,ifelse(iter_num <= (max(iter_num) + min(iter_num))/2, 
                                  'a', 'b'), sep = "")) %>% 
  pivot_longer(x1:x2, names_to = "parametro", values_to = "valor") %>% 
  group_by(parametro, cadena) %>% 
  summarise(media = mean(valor), num = n(), sigma2 = var(valor)) %>% 
  summarise(N = first(num), 
            M = n_distinct(cadena), 
            B = N * var(media), 
            W = mean(sigma2), 
            V_hat = ((N-1)/N) * W + B/N,
            R_hat = sqrt(V_hat/W))

g1 <- cadenas %>% 
  ggplot(aes(x = iter_num, y = x1, color = id)) + 
    geom_path() + sin_leyenda + sin_lineas + 
    ggtitle(paste("Rhat: ", round((diag_corr %>% pull(R_hat))[1], 3), sep = "")) + 
    annotate("rect", xmin = 0, xmax = 500, ymin = -Inf, ymax = Inf, alpha = .2) + 
    annotate("rect", xmin = 0, xmax = 750, ymin = -Inf, ymax = Inf, alpha = .2) + 
    annotate("text", x = c(250, (750+500)/2, (1000+750)/2),
             y = rep(-2.5, 3), 
             label = c("burn-in", "sub 1", "sub 2"))

g2 <- cadenas %>% 
  ggplot(aes(x = iter_num, y = x2, color = id)) + 
    geom_path() + sin_leyenda + sin_lineas + 
    ggtitle(paste("Rhat: ", round((diag_corr %>% pull(R_hat))[2], 3), sep = "")) + 
    annotate("rect", xmin = 0, xmax = 500, ymin = -Inf, ymax = Inf, alpha = .2) + 
    annotate("rect", xmin = 0, xmax = 750, ymin = -Inf, ymax = Inf, alpha = .2) + 
    annotate("text", x = c(250, (750+500)/2, (1000+750)/2),
             y = rep(-2.5, 3), 
             label = c("burn-in", "sub 1", "sub 2"))

g1/g2

```

Mientras que las cadenas que se mezclan bien en el espacio de parámetros (buen
salto en la distribución propuesta) muestran el siguiente estimador de reducción
de escala.

```{r, echo = FALSE, fig.height = 4, out.width = "99%"}

diag_uncorr <- cadenas_bien %>% 
  filter(iter_num > 500) %>% 
  mutate(cadena = paste(id,ifelse(iter_num <= (max(iter_num) + min(iter_num))/2, 
                                  'a', 'b'), sep = "")) %>% 
  pivot_longer(x1:x2, names_to = "parametro", values_to = "valor") %>% 
  group_by(parametro, cadena) %>% 
  summarise(media = mean(valor), num = n(), sigma2 = var(valor)) %>% 
  summarise(N = first(num), 
            M = n_distinct(cadena), 
            B = N * var(media), 
            W = mean(sigma2), 
            V_hat = ((N-1)/N) * W + B/N,
            R_hat = sqrt(V_hat/W)) 


g1 <- cadenas_bien %>% 
  ggplot(aes(x = iter_num, y = x1, color = id)) + 
    geom_path() + sin_leyenda + sin_lineas + 
    ggtitle(paste("Rhat: ", round((diag_uncorr %>% pull(R_hat))[1], 3), sep = "")) + 
      annotate("rect", xmin = 0, xmax = 500, ymin = -Inf, ymax = Inf, alpha = .2) + 
    annotate("rect", xmin = 0, xmax = 750, ymin = -Inf, ymax = Inf, alpha = .2) + 
    annotate("text", x = c(250, (750+500)/2, (1000+750)/2),
             y = rep(-2.5, 3), 
             label = c("burn-in", "sub 1", "sub 2"))

g2 <- cadenas_bien %>% 
  ggplot(aes(x = iter_num, y = x2, color = id)) + 
    geom_path() + sin_leyenda + sin_lineas + 
  ggtitle(paste("Rhat: ", round((diag_uncorr %>% pull(R_hat))[2], 3), sep = "")) + 
      annotate("rect", xmin = 0, xmax = 500, ymin = -Inf, ymax = Inf, alpha = .2) + 
    annotate("rect", xmin = 0, xmax = 750, ymin = -Inf, ymax = Inf, alpha = .2) + 
    annotate("text", x = c(250, (750+500)/2, (1000+750)/2),
             y = rep(-2.5, 3), 
             label = c("burn-in", "sub 1", "sub 2"))

g1/g2
```

#### Ejemplo: Diagnósticos de escala para cantantes de ópera {-}

Ahora consideremos el ejemplo de los cantantes de ópera y para el cual utilizamos
cadenas de longitud 600.

```{r, echo = FALSE, fig.height = 3.5, out.width = "99%"}

sims_rhat_long <- sims_tbl %>% 
  filter(iter_num < 600) %>% 
  filter(iter_num > max(iter_num)/2) %>% 
  mutate(cadena = paste(cadena, ifelse(iter_num <= (max(iter_num) + min(iter_num))/2, 
                                  'a', 'b'), sep = "")) %>% 
  pivot_longer(mu:sigma, names_to = "parametro", values_to = "valor") %>% 
  group_by(parametro, cadena) %>% 
  summarise(media = mean(valor), num = n(), sigma2 = var(valor)) %>% 
  summarise(N = first(num), 
            M = n_distinct(cadena), 
            B = N * var(media), 
            W = mean(sigma2), 
            V_hat = ((N-1)/N) * W + B/N,
            R_hat = sqrt(V_hat/W)) 

g1 <- sims_tbl %>% 
  filter(iter_num < 600) %>% 
  ggplot(aes(x = iter_num, y = mu, color = cadena)) + 
    geom_path() + sin_leyenda + sin_lineas + 
    ggtitle(paste("Rhat: ", round((sims_rhat_long %>% pull(R_hat))[1], 3), sep = "")) + 
      annotate("rect", xmin = 0, xmax = 300, ymin = -Inf, ymax = Inf, alpha = .2) + 
    annotate("rect", xmin = 0, xmax = 450, ymin = -Inf, ymax = Inf, alpha = .2) + 
    annotate("text", x = c(150, (450+300)/2, (600+450)/2),
             y = rep(145, 3), 
             label = c("burn-in", "sub 1", "sub 2"))

g2 <- sims_tbl %>% 
  filter(iter_num < 600) %>% 
  ggplot(aes(x = iter_num, y = sigma, color = cadena)) + 
    geom_path() + sin_leyenda + sin_lineas + 
  ggtitle(paste("Rhat: ", round((sims_rhat_long %>% pull(R_hat))[2], 3), sep = "")) + 
      annotate("rect", xmin = 0, xmax = 300, ymin = -Inf, ymax = Inf, alpha = .2) + 
    annotate("rect", xmin = 0, xmax = 450, ymin = -Inf, ymax = Inf, alpha = .2) + 
    annotate("text", x = c(150, (450+300)/2, (600+450)/2),
             y = rep(4, 3), 
             label = c("burn-in", "sub 1", "sub 2"))

g1/g2
  
```

Estas simulaciones indican problemas en la convergencia de las cadenas. Los
gráficos de traza sugieren incrementar el número de simulaciones pues parece que
eventualmente todas las cadenas alcanzan el estado estacionario. Si
incrementamos a 1000 simulaciones por cadena:

```{r, echo = FALSE, fig.height = 3.5, out.width = "99%"}

sims_rhat_long <- sims_tbl %>% 
  filter(iter_num < 1000) %>% 
  filter(iter_num > max(iter_num)/2) %>% 
  mutate(cadena = paste(cadena, ifelse(iter_num <= (max(iter_num) + min(iter_num))/2, 
                                  'a', 'b'), sep = "")) %>% 
  pivot_longer(mu:sigma, names_to = "parametro", values_to = "valor") %>% 
  group_by(parametro, cadena) %>% 
  summarise(media = mean(valor), num = n(), sigma2 = var(valor)) %>% 
  summarise(N = first(num), 
            M = n_distinct(cadena), 
            B = N * var(media), 
            W = mean(sigma2), 
            V_hat = ((N-1)/N) * W + B/N,
            R_hat = sqrt(V_hat/W)) 

g1 <- sims_tbl %>% 
  filter(iter_num < 1000) %>% 
  ggplot(aes(x = iter_num, y = mu, color = cadena)) + 
    geom_path() + sin_leyenda + sin_lineas + 
    ggtitle(paste("Rhat: ", round((sims_rhat_long %>% pull(R_hat))[1], 3), sep = "")) + 
      annotate("rect", xmin = 0, xmax = 500, ymin = -Inf, ymax = Inf, alpha = .2) + 
    annotate("rect", xmin = 0, xmax = 750, ymin = -Inf, ymax = Inf, alpha = .2) + 
    annotate("text", x = c(250, (750+500)/2, (1000+750)/2),
             y = rep(145, 3), 
             label = c("burn-in", "sub 1", "sub 2"))

g2 <- sims_tbl %>% 
  filter(iter_num < 1000) %>% 
  ggplot(aes(x = iter_num, y = sigma, color = cadena)) + 
    geom_path() + sin_leyenda + sin_lineas + 
  ggtitle(paste("Rhat: ", round((sims_rhat_long %>% pull(R_hat))[2], 3), sep = "")) + 
      annotate("rect", xmin = 0, xmax = 500, ymin = -Inf, ymax = Inf, alpha = .2) + 
    annotate("rect", xmin = 0, xmax = 750, ymin = -Inf, ymax = Inf, alpha = .2) + 
    annotate("text", x = c(250, (750+500)/2, (1000+750)/2),
             y = rep(4, 3), 
             label = c("burn-in", "sub 1", "sub 2"))

g1/g2
  
```
Incrementar las simulaciones nos da valores de $\hat{R}$ cercanos a uno, lo
cual indica que este diagnóstico es aceptable. Nota que en este caso fue
suficiente incrementar la longitud de las cadenas para tener mejores resultados.

```{block, type = 'comentario'}

Problemas con $\hat{R}.$ El estimador de reducción de escala funciona bien 
para monitorear estimadores y cantidades de interés basados en medias y 
varianzas, o bien, cuando la distribución es simétrica y cercana a una
Gaussiana. Es decir, colas ligeras. Sin embargo, para percentiles, o
distribuciones lejos del supuesto de normalidad no es un buen indicador. Es por
esto que también se recomienda incorprorar transformaciones que nos permitan generar un
buen estimador.

```

Revisa el artículo [@vehtariRank] donde se hablan de las limitantes y cálculos
alternativos basados en estimador $\hat R.$


### Número efectivo de simulaciones  {-}

Una vez que tenemos una muestra representativa de la distribución posterior,
nuestro objetivo es asegurarnos de que la muestra es lo suficientemente grande
para producir estimaciones estables y precisas de los resúmenes que nos
interesan.

La idea es que si las $n$ simulaciones dentro de cada cadena en verdad son
realizaciones independientes entonces la estimación de $B$ sería un estimador insesgado 
de $\mathbb{V}(\phi).$ En esta situación tendríamos $n \cdot m$ 
realizaciones de la distribución que queremos calcular. Sin embargo, la
correlación entre las muestras hacen que $B$ sea mayor que $\mathbb{V}(\phi)$ en
promedio.

Una manera para definir el tamaño efectivo de simulaciones es por medio del estudio
del estimador $\bar{\phi}_{\cdot\cdot},$ que en general esperamos sea una buena
aproximación para $\mathbb{E}(\phi).$ Como bien sabemos, podemos esperar el
siguiente comportamiento si las muestras son independientes

$$\mathbb{V}(\bar{\phi}_{\cdot\cdot}) = \frac{\mathbb{V}(\phi)}{m\cdot n},$$

sin embargo la correlación en las cadenas implica el denominador de arriba
realmente sea una fracción del total de muestras, digamos $\lambda.$ De tal forma que 
el número efectivo de simulaciones es 

$$N_{\mathsf{eff}} = \lambda \cdot (m \, n),$$

donde 

$$ \lambda = \frac{1}{\sum_{t = -\infty}^\infty \rho_t} = \frac{1}{1 + 2 \sum_{t = 1}^\infty  \rho_t}\,,$$

y $\rho_t$ denota la **auto-correlación** con retraso en $t$ unidades de tiempo. 
Para estimar $\rho_t$ partimos de nuestro estimador $\hat{\mathbb{V}}(\phi)^+;$
y utilizamos el *variograma* $V_t$ para cada retraso $t$

$$V_t = \frac{1}{m (n - t)} \sum_{j = 1}^m \sum_{i = t + 1}^n (\phi_{i,j} - \phi_{i-t, j})^2\,.$$

Despúes utilizamos la igualdad $\mathbb{E}(\phi_{i} - \phi_{i-t})^2 = 2 (1 - \rho_t) \mathbb{V}(\phi),$
para encontrar un estimador 

$$\hat \rho_t = 1 - \frac{V_t}{2 \, \hat{\mathbb{V}}(\phi)^+} \, . $$

La mayor dificultad que presenta el estimador es considerar *todos* los retrasos
posibles. Eventualmente agotaremos la longitud de las cadenas para ello. Por
otro lado, para $t$ eventualmente grande nuestros estimadores del variograma
$V_t$ serán muy ruidosos (¿por qué?). En la práctica truncamos la serie de
acuerdo a las observaciones en [@geyer]. La serie tiene la propiedad de que para
cada par $\rho_{2 t} + \rho_{2 t + 1} > 0.$ Por lo tanto, la serie se trunca 
cuando observamos $\hat \rho_{2 t} + \hat \rho_{2 t + 1} < 0$ para dos retrasos
sucesivos. Si denotamos por $T$ el tiempo de paro,  el estimador para el número
efectivo de simulaciones es

$$\hat N_{\mathsf{eff}} = \frac{m \, n}{1 + 2 \sum_{t = 1}^T \hat  \rho_t}\,.$$


**Observaciones:** El **tamaño efectivo de simulaciones** nos ayuda a monitorear
lo siguiente. Si las simulaciones fueran independientes $N_{\textsf{eff}}$ sería
el número total de simulaciones; sin embargo, las simulaciones de MCMC suelen
estar correlacionadas, de modo que cada iteración de MCMC es menos informativa
que si fueran independientes.

Por ejemplo Si graficaramos simulaciones independientes, esperaríamos valores de 
autocorrelación chicos:

```{r, out.width = "99%", fig.height = 2}
library(forecast)
ggAcf(rgamma(1000,1,1)) + sin_lineas
```
Sin embargo, los valores que simulamos tienen el siguiente perfil de
autocorrelación:

```{r, out.width = "99%", fig.height = 2}
sigma_metro_sims <- sims_tbl %>% filter(cadena==4) %>% pull(mu)
ggAcf(sigma_metro_sims) + sin_lineas
```

Usualmente nos gustaría obtener un tamaño efectivo de al menos $100$ (para
cálculo de medias y varianzas posteriores). Esta cantidad usualmente se reporta
en el software, por ejemplo en Stan, y es necesario checarlo.

En nuestro primer ejemplo (Metropolis-Hastings con muestras altamente
correlacionadas y propuesta cerca del *óptimo*) calculamos para cada caso:

```{r}
library("posterior")

ess_basic(cadenas %>% filter(iter_num > 500) %>% 
  select(id, iter_num, x1) %>% 
  mutate(id = paste("c",as.character(id), sep = "")) %>% 
  pivot_wider(names_from = id, values_from = x1) %>% 
  select(-iter_num))

ess_basic(cadenas_bien %>% filter(iter_num > 500) %>% 
  select(id, iter_num, x1) %>% 
  mutate(id = paste("c",as.character(id), sep = "")) %>% 
  pivot_wider(names_from = id, values_from = x1) %>% 
  select(-iter_num))

```

donde vemos que para el muestreador con mejor mezcla tenemos un número bueno de 
muestras para la estimación del promedio de $x_1.$

En el caso para las simulaciones del modelo posterior de los cantantes vemos que
para $\mu$ tenemos valores de $N_{\mathsf{eff}}$ iguales a

```{r}
ess_basic(sims_tbl %>% 
  filter(iter_num > 300) %>% 
  filter(iter_num <= 600) %>% select(cadena, iter_num, mu) %>% 
  mutate(cadena = paste("c",as.character(cadena), sep = "")) %>% 
  pivot_wider(names_from = cadena, values_from = mu) %>% 
  select(-iter_num))

ess_basic(sims_tbl %>% 
  filter(iter_num > 500) %>% 
  filter(iter_num <= 1000) %>% select(cadena, iter_num, mu) %>% 
  mutate(cadena = paste("c",as.character(cadena), sep = "")) %>% 
  pivot_wider(names_from = cadena, values_from = mu) %>% 
  select(-iter_num))
```

y para $\sigma$ tenemos

```{r}
ess_basic(sims_tbl %>% 
  filter(iter_num > 300) %>% 
  filter(iter_num <= 600) %>% select(cadena, iter_num, sigma) %>% 
  mutate(cadena = paste("c",as.character(cadena), sep = "")) %>% 
  pivot_wider(names_from = cadena, values_from = sigma) %>% 
  select(-iter_num))

ess_basic(sims_tbl %>% 
  filter(iter_num > 500) %>% 
  filter(iter_num <= 1000) %>% select(cadena, iter_num, sigma) %>% 
  mutate(cadena = paste("c",as.character(cadena), sep = "")) %>% 
  pivot_wider(names_from = cadena, values_from = sigma) %>% 
  select(-iter_num))
```

Lo cual nos dice que aunque el factor de disminución de escala $\hat R$ presenta
un buen diágnostico, nuestras estimaciones Monte-Carlo aún pueden mejorarse si
consideramos el nivel de referencia igual a 100.

## Diagnósticos para HMC {-}

Ahora mencionaremos los diagnósticos particulares que nos ayudan a identificar
problemas en la evolución de HMC. Estos diagnósticos son guías para identificar
problemas y no son útiles para identificar transiciones óptimas. En la práctica
se utilizan en conjunto con los que ya hemos visto anteriormente, pues su
validez descansa en el supuesto de que HMC permite explorar eficientemente la
densidad posterior. 

### Transiciones divergentes {-}

Como discutido anteriormente, el _software_ que utilizaremos en este curso es
Stan [@stan] y usa como muestreador una versión computacionalmente eficiente 
de Monte Carlo Hamiltoniano (HMC). ESto lo logra, al simular trayectorías 
en el sistema Hamiltoniano que describe la posición (la variable aleatoria de
interes) y la incercia (una variable auxiliar). La evolución del sistema dinámico 
se hace por medio de una aproximación numérica utilizando un tiempo ficticio.
Para esto, se escoge una longitud de paso que determina cuánto se mueve una
particula en una fracción de tiempo. Este longitud de paso controla la
resolución del muestreador.

Pueden existir problemas con esta resolución cuando hay caracteristicas de la
densidad objetivo que necesitan de una trayectoria mas fina para poder ser
exploradas. Si el muestreador no es capaz de explorar estas regiones, entonces
la estimación Monte Carlo será una estimación sesgada. Lo bueno, es que 
este fenómeno se puede explorar de manera computacional pues lo vemos como 
trayectorias divergentes. En particular, nos interesa saber si éstas existen
después del periodo de calentamiento.

En Stan podemos ajustar esto con un parámetro del algoritmo que se llama
`adapt_delta`. Sin embargo, esto puede no ser siempre exitoso y podría ser 
que necesitemos cambiar la forma en que parametrizamos nuestro modelo. El
objetivo de esto es expresar el mismo modelo en términos de una geometría mas
sencilla. Esta situación es bastante común en modelo jerárquicos y veremos mas
adelante un ejemplo de esto.

### Longitud máxima de exploración {-}

Hay otras alertas en Stan que se refieren a la longitud del árbol que utiliza el
simulador de la trayectoria. En particular, el algoritmo de NUTS (No U-Turn
Sampler) es la configuración que simula dicha trayectoria. Esto lo hace al
simular la trayectoria evolucionando el tiempo hacia adelante y hacia atrás.
Esto genera una árbol binario con el cual buscamos explorar lo mas posible el
soporte de la distribución. El algoritmo termina si: i) se satisface el criterio
de vuelta en U (ver [@nuts] para mas detalles); o ii) si la longitud máxima del
árbol binario se alcanza.

La alerta es en general menos preocupante que una alerta de transición
divergente y habla mas sobre la eficiencia numérica del algoritmo. La opción
`max_depth` nos ayuda a controlar este comportamiento en Stan.

### Fracción de Pérdida de Información {-}

El muestreador regresa una variable llamada `energy__` que usamos para
diagnosticar la precision de un muestreado HMC. Si la desviación estándar de la
energia es mayor que $\sqrt{p}/2,$ entonces el muestreador será ineficiente para
generar trayectorias que exploren bien las curvas de nivel del Hamiltoniano, y
en consequencia, proponer transiciones que exploren bien la distribución
objetivo. Este comportamiento se puede encontrar en situaciones con posteriores
con colas pesadas y se puede solucionar, a veces, reparametrizando el problema o
incrementando el número de simulaciones. Puedes consultar [@bfmi] para mas
detalles en este diagnóstico.