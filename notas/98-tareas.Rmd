# Tareas {-}

```{r, include=FALSE, message=FALSE}
library(tidyverse)
library(patchwork)
library(scales)
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning=FALSE, 
                      fig.align = 'center', fig.width = 5, fig.height=3, cache = TRUE)
comma <- function(x) format(x, digits = 2, big.mark = ",")
theme_set(theme_linedraw())
color.blues <- c(NA,"#BDD7E7", "#6BAED6", "#3182BD", "#08519C", "#074789", "#063e77", "#053464")
color.itam  <- c("#00362b", "#00503f", "#006953", "#008367", "#009c7b", "#00b68f")


sin_lineas <- theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank())
sin_leyenda <- theme(legend.position = "none")
sin_ejes <- theme(axis.ticks = element_blank(), 
        axis.text = element_blank())
```

## Integración Monte Carlo y Cadenas de Markov {-}

1. Utiliza el método Monte Carlo para aproximar 
$$\int_1^2 \frac{e^{-x^2/2}}{\sqrt{2 \pi}} \, \text{d}x \, .$$
*Hint: * reescribe la integral en términos de un problema de integración con
respecto a una densidad $\pi(x)$ y una función resumen $f(x).$

1. Marginalización Monte Carlo es una técnica para calcular densidades 
marginales cuando simulamos de una densidad conjunta. Sea $\pi(x,y)$ una
densidad conjunta y sea $\pi(x) = \int \pi(x, y) \text{d} y \,,$ la densidad
marginal de $x.$ 
    
    - Sea $w(x)$ una densidad arbitraria. Prueba que $$\int \int
    \frac{\pi(x^\star, y) w(x)}{\pi(x, y)} \, \pi(x, y) \,\text{d}x\, \text{d}y
    \, = \pi(x^\star)\,.$$
    
    - Argumenta cómo podríamos utilizar la formulación de arriba para constuir un 
    estimador Monte Carlo de la densidad marginal. 
    
    - En el contexto de los puntos anteriores, sean $x|y \sim \mathsf{Gamma}(y,
    1)$ y $y \sim \mathsf{Exp}(1).$ Utiliza la técnica de arriba para calcular
    la densidad marginal de x. *Hint:* Considera un soporte amplio para $x.$ Por
    ejemplo, considera aproximar la densidad en 100 puntos equidistantes en el
    intervalo $(0,10).$

2. Implementa el método Metropolis para muestrear de una
$\mathsf{Poisson}(\lambda).$ Prueba con $\lambda = 3, 5,$ y $10.$ Para la 
distribución de propuesta utiliza una caminata aleatoria sobre los enteros.
*Hint: * recuerda lo que hicimos con las islas del vendedor.

2. Sea $X \sim \mathsf{N}(0,1).$ Implementa el método Metropolis para generar
muestras de $X.$ Como distribución de propuesta utiliza una normal con varianza 
igual a $\sigma^2.$ Prueba varios valores para $\sigma.$ Como caso particular
considera $\sigma = 2.38,$ ¿cómo se compara la tasa de aceptación y las
trayectorias con estos distintos valores?

3. Modelo logístico. (*Tomado de* @robertCasella). Este ejercicio considera 
la situación del lanzamiento del Challenger en 1986. La explosión se originó 
por la falla de un anillo que sella ciertos componentes de la nave espacial.
Se cree que el accidente se originó por las bajas temperaturas al momento de 
lanzamiento. Se considera que la probabilidad de falla aumenta cuando la 
temperatura decrece. Se tienen datos históricos
```{r, echo = FALSE}
datos <- tibble(falla = c(1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,1, 
                          0, 0, 0, 0, 0), 
                temperatura = c(53, 57, 58, 63, 66, 67, 67, 67, 68, 69, 70, 70, 
                                70, 70, 72, 73, 75, 75, 76, 76, 78, 79, 81)) %>% 
     mutate( temperatura = (temperatura - 32) * 5/9)

datos %>% head()
```
    
y se modela la probabilidad de falla como 
$$\mathbb{P}(Y = 1 | X ) = p(X) = \frac{\exp(\alpha + \beta X)}{1 + \exp(\alpha + \beta X)} \, .$$

Consideramos la muestra como observaciones 
$$Y_i \sim \mathsf{Bernoulli}(p(X_i))\,,$$
con distribución previa de la forma
$$\pi(\alpha, \beta) = \pi(\alpha) \, \pi(\beta) \propto \alpha^{a - 1}\exp\left(- b \, \alpha \right) \,.$$
Es decir, se asume $\alpha \sim \mathsf{Gamma}(a, b),$ y una distribución
proporcional a una constante para $\beta.$

El parámetro $\alpha$ define la probabilidad de falla considerando una
temperatura de $0^\circ$ centígrados. De acuerdo a estudios anteriores esta 
probabilidad de falla se encuentra en el intervalo de $(85\%, 99\%)$.

Construye una distribución Gamma adecuada para capturar este intervalo con 
$90\%$ de probabilidad. Esta densidads la denotaremos como $\pi(\alpha | a_0,
b_0).$

```{r, echo = FALSE, out.width = '99%', fig.width = 7, include = FALSE}
# renv::install("nleqslv")
library("nleqslv")

logit <- function(p){
    log(p/(1-p))
}

probs  <- c(.85, .99)
limits <- logit(probs)
p_cota <- .1

gamma.limits <- function(x){
  # reparametrizamos para que el problema sea mas "fácil" en términos numéricos.
  log_alpha <- x[1]
  log_beta  <- x[2]
  
  # definimos las cotas de probabilidad
  p_cota <- 0.01
  c(    pgamma(limits[1], exp(log_alpha), rate = exp(log_beta)) - p_cota,
    1 - pgamma(limits[2], exp(log_alpha), rate = exp(log_beta)) - p_cota)
}

initial_guess <- c(1, 1)

results <- nleqslv(initial_guess, gamma.limits)
params.prior <- exp(results$x)

g_guess <- tibble(tasa = rgamma(5000, exp(initial_guess[1]), exp(initial_guess[2]))) %>%
    mutate(prob = exp(tasa)/(1 + exp(tasa))) %>% 
  ggplot(aes(x = prob)) + 
    geom_histogram() + 
    ggtitle(paste("(Guess) a = ", round(exp(initial_guess[1]), 3), 
                  ", b = ", round(exp(initial_guess[2]), 3), sep = "")) + 
    sin_lineas

g_elicited <- tibble(tasa = rgamma(5000, params.prior[1], rate = params.prior[2])) %>%
    mutate(prob = exp(tasa)/(1 + exp(tasa))) %>% 
  ggplot(aes(x = prob)) + 
    geom_histogram() + 
    ggtitle(paste("(Elicited) a = ", round(params.prior[1], 3), 
                  ", b = ", round(params.prior[2], 3), sep = "")) + sin_lineas

g_guess + g_elicited

```

```{r, echo = FALSE, include = FALSE}

crear_log_p <- function(datos){
  log_p <- function(pars){
    alpha = pars[1]
    beta  = pars[2]
    n <- length(datos)
    # ve la ecuación del ejercicio anterior
    lp  <- alpha  + beta * datos$temperatura
    p   <- exp(lp)/(1+exp(lp))
    log_verosim <- datos$falla * log(p) +  (1 - datos$falla) * log(1 - p)
    sum(log_verosim)
  }  
  log_p
}
log_p <- crear_log_p(datos)

initial_guess_mle <- c(rnorm(1), rnorm(1))

res <- optim(initial_guess_mle, log_p, control = list(fnscale = -1, maxit = 1000), 
             method = "Nelder-Mead")
res$par
```

Ahora, utiliza el método de Metropolis-Hastings para generar muestras del 
modelo de probabilidad posterior para $\alpha, \beta.$ Para esto considera
una densidad de propuesta
$$q(\alpha, \beta) = \pi(\alpha|a_0, b_0) \, \phi(\beta)\, ,$$
donde $\phi(\beta)$ es la densidad de una $\mathsf{N}(\hat \beta_{\mathsf{MLE}},
\sigma^2).$ Nota que las propuestas incorporan la distribución previa de
$\alpha.$ Por otro lado, la propuesta esta definida en términos del estimador de
máxima verosimilitud de $\beta.$ Con esta elección escribe la probabilidad de
aceptación de Metropolis-Hastings y corre el algoritmo de Metropolis-Hastings para 
un número suficiente de simulaciones (prueba distintos valores de $\sigma^2$).

## Modelos y Simulación en `Stan`

1. Implementa el modelo que vimos en clase sobre las estaturas de cantantes de
ópera. Usa un modelo diferente para que uno de los supuestos en la distribución
previa.

1. Implementa el modelo para los datos del Challenger que se presentaron en los
ejercicios anteriores.

